{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "isolated-zoning",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Collecting pytorch-lightning==1.6.0\n",
      "  Using cached pytorch_lightning-1.6.0-py3-none-any.whl (582 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for json\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/users1/musenips/master_venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f31d6d-7a90-4e46-a10d-06133675a278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp\n",
      "  Downloading allennlp-2.10.0-py3-none-any.whl (729 kB)\n",
      "\u001b[K     |████████████████████████████████| 729 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=3.6.0\n",
      "  Downloading h5py-3.7.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 115.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch<1.12.0,>=1.10.0\n",
      "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[K     |█████████████▋                  | 318.0 MB 8.1 MB/s eta 0:00:5404MB 145.3 MB/s eta 0:00:05/s eta 0:00:04           | 274.1 MB 8.1 MB/s eta 0:00:59"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████    | 655.7 MB 158.2 MB/s eta 0:00:01 427.3 MB 49.6 MB/s eta 0:00:07     |██████████████████▋             | 436.9 MB 49.6 MB/s eta 0:00:07█████████████████             | 444.6 MB 49.6 MB/s eta 0:00:07�███████▋            | 461.0 MB 167.2 MB/s eta 0:00:02 167.2 MB/s eta 0:00:02         | 503.5 MB 167.2 MB/s eta 0:00:02��██         | 539.8 MB 167.2 MB/s eta 0:00:02��████████████████        | 562.6 MB 158.2 MB/s eta 0:00:02��██████████████████▋    | 648.8 MB 158.2 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 750.6 MB 1.5 kB/s \n",
      "\u001b[?25hCollecting traitlets>5.1.1\n",
      "  Downloading traitlets-5.3.0-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 121.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wandb<0.13.0,>=0.10.0\n",
      "  Using cached wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting scipy>=1.7.3\n",
      "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.2 MB 122.7 MB/s eta 0:00:01        | 29.9 MB 122.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest>=6.2.5\n",
      "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 22.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fairscale==0.4.6\n",
      "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
      "\u001b[K     |████████████████████████████████| 248 kB 108.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorboardX>=1.2\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 102.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision<0.13.0,>=0.8.1\n",
      "  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 21.1 MB/s eta 0:00:01    |█████▍                          | 3.5 MB 21.1 MB/s eta 0:00:01:00:01\n",
      "\u001b[?25hCollecting more-itertools>=8.12.0\n",
      "  Downloading more_itertools-8.13.0-py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 178 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3\n",
      "  Downloading cached_path-1.1.5-py3-none-any.whl (26 kB)\n",
      "Collecting protobuf==3.20.0\n",
      "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 78.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.62\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 3.7 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.21.4\n",
      "  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.0 MB 94.7 MB/s eta 0:00:01█████▊                 | 7.9 MB 94.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=2.28\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 939 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typer>=0.4.1\n",
      "  Downloading typer-0.6.1-py3-none-any.whl (38 kB)\n",
      "Collecting scikit-learn>=1.0.1\n",
      "  Downloading scikit_learn-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.4 MB 99.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill>=0.3.4\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 3.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor==1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting lmdb>=1.2.1\n",
      "  Downloading lmdb-1.3.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 134.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers<4.21,>=4.1\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Collecting huggingface-hub>=0.0.16\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nltk>=3.6.5\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 111.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock<3.8,>=3.3\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting sentencepiece>=0.1.96\n",
      "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 21.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting base58>=2.1.1\n",
      "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting spacy<3.4,>=2.1.0\n",
      "  Downloading spacy-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 35.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rich==12.1\n",
      "  Downloading rich-12.1.0-py3-none-any.whl (229 kB)\n",
      "\u001b[K     |████████████████████████████████| 229 kB 115.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonnet>=0.10.0\n",
      "  Downloading jsonnet-0.18.0.tar.gz (592 kB)\n",
      "\u001b[K     |████████████████████████████████| 592 kB 91.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pygments<3.0.0,>=2.6.0\n",
      "  Downloading Pygments-2.12.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 72.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 3.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-storage<3.0,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 119.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3<2.0,>=1.0\n",
      "  Downloading boto3-1.24.31-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 53.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.28.0,>=1.27.31\n",
      "  Downloading botocore-1.27.31-py3-none-any.whl (9.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.0 MB 15.9 MB/s eta 0:00:01     |██████████████████▌             | 5.2 MB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 3.2 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[K     |████████████████████████████████| 247 kB 52.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 118.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 100.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 3.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
      "\u001b[K     |████████████████████████████████| 167 kB 21.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 123.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 117.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting six>=1.9.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (37 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[K     |████████████████████████████████| 682 kB 96.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging>=20.9\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 132.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 3.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 91.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 3.6 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 2.3 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 4.2 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting tomli>=1.0.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 2.7 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 116.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 4.9 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
      "\u001b[K     |████████████████████████████████| 659 kB 96.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer>=0.4.1\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: setuptools in /home/users1/musenips/master_venv/lib/python3.10/site-packages (from spacy<3.4,>=2.1.0->allennlp) (57.4.0)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 102.9 MB/s eta 0:00:01  | 6.3 MB 102.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 22.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 748 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 85.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
      "\u001b[K     |████████████████████████████████| 457 kB 89.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 116.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 108.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 23.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 105.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.7.2-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 98.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting psutil>=5.0.0\n",
      "  Downloading psutil-5.9.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "\u001b[K     |████████████████████████████████| 282 kB 119.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 109.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'rich' candidate (version 12.1.0 at https://files.pythonhosted.org/packages/bc/be/1ace556afa0cf17599c2a631b04b280ae7502a9cf942c47fd66ca9ab5134/rich-12.1.0-py3-none-any.whl#sha256=b60ff99f4ff7e3d1d37444dee2b22fdd941c622dbc37841823ec1ce7f058b263 (from https://pypi.org/simple/rich/) (requires-python:>=3.6.2,<4.0.0))\n",
      "Reason for being yanked: Broken dependencies. Please upgrade to 12.2.0 or later\u001b[0m\n",
      "Building wheels for collected packages: fairscale, termcolor, jsonnet, promise, pathtools, sacremoses\n",
      "  Building wheel for fairscale (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307225 sha256=d7c9d77f899bf572fae6365e622c9b67e76404fb7068e1cd7e11883949c9a363\n",
      "  Stored in directory: /home/users1/musenips/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=6e80456741b4f7d33c8ba077e84d037ebfbab080f36995ae3069ff33286a4bb2\n",
      "  Stored in directory: /home/users1/musenips/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l|"
     ]
    }
   ],
   "source": [
    "!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "behind-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import textwrap\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    ")\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outdoor-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "rcParams['figure.figsize'] = 16, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "buried-above",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41d7c4-2642-465e-ab34-29be98953942",
   "metadata": {},
   "source": [
    "Commands for preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13a8262-fdca-49a1-b1d8-ade71d88de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://gitlab.com/shimorina/webnlg-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825680a2-76c2-4e41-a112-f6aed58623b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mv 'webnlg-dataset/release_v3.0/en' 'webnlg_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0d7dfc-2364-4efd-bfea-53ff72a3090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r --interactive=never 'webnlg-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c43727-e442-4b4b-9873-9e48ae817d52",
   "metadata": {},
   "source": [
    "Commands for removing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a846b0ac-95dd-4de6-bb55-38c2260c1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r --interactive=never 'webnlg_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8689e820-475e-42ba-a36f-535401127ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm web*\\.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a9a7cd-f301-4bfe-a309-93d9badb77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "train_path = \"train/**/*\"\n",
    "dev_path = \"dev/**/*\"\n",
    "test_path = \"test/rdf-to-text-generation-test-data-with-refs-en\"\n",
    "sets = [train_path, dev_path, test_path]\n",
    "\n",
    "for s in sets:\n",
    "    files = glob.glob(os.getcwd() + \"/webnlg_data/\" + s + \".xml\", recursive=True)    \n",
    "    inputs = list()\n",
    "    outputs = list()\n",
    "    for file in files:\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()    \n",
    "        for sub_root in root:\n",
    "            for entry in sub_root:\n",
    "                for element in entry:\n",
    "                    if \"modifiedtripleset\" in str(element):\n",
    "                        inp = \" && \".join([triple.text for triple in element])\n",
    "                        # inp = \" && \".join([\"|\".join([e.strip(\"\\\"\") for e in triple.text.split(\"|\")]) for triple in element])\n",
    "                    if \"lex\" in str(element):\n",
    "                        out = element.text\n",
    "                        inputs.append(inp)\n",
    "                        outputs.append(out)\n",
    "\n",
    "    mdata_dct={\"input_text\":[], \"target_text\":[]}\n",
    "    for i, _ in enumerate(inputs):\n",
    "        mdata_dct['input_text'].append(inputs[i])\n",
    "        mdata_dct['target_text'].append(outputs[i])\n",
    "\n",
    "\n",
    "    df=pd.DataFrame(mdata_dct)\n",
    "    df.to_csv('webNLG2020_' + s.split(\"/\")[0] + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13329b76-9ffc-487d-913b-ca46769b4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'webNLG2020_train.csv'\n",
    "DEV_PATH = 'webNLG2020_dev.csv'\n",
    "TEST_PATH = 'webNLG2020_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naval-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df = pd.read_csv(DEV_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a5e26ae-d246-4f45-b383-9bfbc2a96a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AmeriGas | country | United_States &amp;&amp; AmeriGas...</td>\n",
       "      <td>AmeriGas was founded in the United States on 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AmeriGas | country | United_States &amp;&amp; AmeriGas...</td>\n",
       "      <td>AmeriGas, founded on 01-01-1959, works in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AmeriGas | country | United_States &amp;&amp; AmeriGas...</td>\n",
       "      <td>AmeriGas, whose home country is the United Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AmeriGas | country | United_States &amp;&amp; AmeriGas...</td>\n",
       "      <td>AmeriGas is based in the United States and emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AmeriGas | country | United_States &amp;&amp; AmeriGas...</td>\n",
       "      <td>AmeriGas, whose current employment is 8,500, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  AmeriGas | country | United_States && AmeriGas...   \n",
       "1  AmeriGas | country | United_States && AmeriGas...   \n",
       "2  AmeriGas | country | United_States && AmeriGas...   \n",
       "3  AmeriGas | country | United_States && AmeriGas...   \n",
       "4  AmeriGas | country | United_States && AmeriGas...   \n",
       "\n",
       "                                         target_text  \n",
       "0  AmeriGas was founded in the United States on 1...  \n",
       "1  AmeriGas, founded on 01-01-1959, works in the ...  \n",
       "2  AmeriGas, whose home country is the United Sta...  \n",
       "3  AmeriGas is based in the United States and emp...  \n",
       "4  AmeriGas, whose current employment is 8,500, h...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.concat([train_df, dev_df, test_df])\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4abd7a96-67a2-4c15-b95e-bd5aa8184401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35426, 2) (4464, 2) (5150, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, dev_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b46bf4-c609-4495-b3ce-472bf5461d5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'allennlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4ef14e5cc998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpacyWordSplitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWordTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq_encoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytorchSeq2SeqWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m encoder = PytorchSeq2SeqWrapper(\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'allennlp'"
     ]
    }
   ],
   "source": [
    "from allennlp.data.tokenizers.word_tokenizer import SpacyWordSplitter, WordTokenizer\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "\n",
    "tokenizer = WordTokenizer()\n",
    "encoder = PytorchSeq2SeqWrapper(\n",
    "    torch.nn.LSTM(EN_EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "376cd62a-c628-4be1-9906-bdffb3980e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_lens = list()\n",
    "summary_lens = list()\n",
    "for idx, row in full_data.iterrows():\n",
    "    rdf_encoding = tokenizer(row[\"input_text\"])\n",
    "    rdf_lens.append(len(rdf_encoding.tokens()))\n",
    "    summary_encoding = tokenizer(row[\"target_text\"])\n",
    "    summary_lens.append(len(summary_encoding.tokens()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64244a3a-6005-4f55-94f6-a211f7ef39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(max(rdf_lens),max(summary_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa4e221-a887-40b0-a7b4-494082ce379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/users1/musenips/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login '47616056d2ebbf7dea86db60a5fc58145cd234fc'  ## andere Lösung finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "contemporary-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebNLGDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: WordTokenizer(),\n",
    "        text_max_token_len: int = 263,\n",
    "        summary_max_token_len: int = 147\n",
    "    ):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text_max_token_len = text_max_token_len\n",
    "        self.summary_max_token_len = summary_max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        \n",
    "        text = data_row['input_text']\n",
    "        \n",
    "        text_encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=self.text_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        summary_encoding = tokenizer(\n",
    "            data_row[\"target_text\"],\n",
    "            max_length=self.text_max_token_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = summary_encoding[\"input_ids\"]\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return dict(\n",
    "            text=text,\n",
    "            summary=data_row[\"target_text\"],\n",
    "            text_input_ids=text_encoding[\"input_ids\"].flatten(),\n",
    "            text_attention_mask=text_encoding[\"attention_mask\"].flatten(),\n",
    "            labels=labels.flatten(),\n",
    "            labels_attention_mask=summary_encoding[\"attention_mask\"].flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "subsequent-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebNLGDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        dev_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: WordTokenizer(),\n",
    "        batch_size: int = 8,\n",
    "        text_max_token_len: int = 263,\n",
    "        summary_max_token_len: int = 147\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = train_df\n",
    "        self.dev_df = dev_df\n",
    "        self.test_df = test_df\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_max_token_len = text_max_token_len\n",
    "        self.summary_max_token_len = summary_max_token_len\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = WebNLGDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_len,\n",
    "            self.summary_max_token_len\n",
    "        )\n",
    "        self.test_dataset = WebNLGDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_len,\n",
    "            self.summary_max_token_len\n",
    "        )\n",
    "        self.dev_dataset = WebNLGDataset(\n",
    "            self.dev_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_len,\n",
    "            self.summary_max_token_len\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=56\n",
    "        )\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dev_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=56\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=56\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "roman-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "data_module = WebNLGDataModule(train_df, test_df, dev_df, tokenizer, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "color-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebNLGModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\n",
    "                      beam_size=8,)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n",
    "        \n",
    "        output = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "        \n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"text_input_ids\"]\n",
    "        attention_mask = batch[\"text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "        \n",
    "        loss, outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"text_input_ids\"]\n",
    "        attention_mask = batch[\"text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "        \n",
    "        loss, outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"text_input_ids\"]\n",
    "        attention_mask = batch[\"text_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
    "        \n",
    "        loss, outputs = self(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=labels_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=0.0001)         ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "surgical-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WebNLGModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "individual-advance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=wandb.run.dir,\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "logger = WandbLogger('WebNLG')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # accelerator = 'dp',\n",
    "    logger=logger,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=[7],                                     ###############\n",
    "    progress_bar_refresh_rate=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30671205-cbac-4140-bca6-0d372be0100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!TOKENIZERS_PARALLELISM=false\n",
    "# !CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-tuesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7921ceabfc174d9697e1e050cbb176ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c349cde9d6044c26a5acfb3385b6ba07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dca5d7351444a2ea2775d9e7ae09979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 8856: val_loss reached 1.32223 (best 1.32223), saving model to \"/home/users1/musenips/D2T_AX_WebNLG/checkpoints/best-checkpoint-v1.ckpt\" as top 1\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fc7505b-430d-4659-895d-6171b407eff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 18 13:41:04 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.57       Driver Version: 515.57       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 49%   76C    P2   260W / 250W |  11009MiB / 11264MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:26:00.0 Off |                  N/A |\n",
      "| 22%   31C    P8     8W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |                  N/A |\n",
      "| 22%   37C    P2    61W / 250W |  10520MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |                  N/A |\n",
      "| 37%   59C    P2   243W / 250W |   9122MiB / 11264MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 45%   71C    P2   226W / 250W |   8752MiB / 11264MiB |     88%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  On   | 00000000:A1:00.0 Off |                  N/A |\n",
      "| 22%   29C    P8    14W / 250W |  10462MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 22%   33C    P2    58W / 250W |  10520MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce ...  On   | 00000000:E1:00.0 Off |                  N/A |\n",
      "| 22%   25C    P8    19W / 250W |      3MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    142819      C   ...env/openprompt/bin/python     3699MiB |\n",
      "|    0   N/A  N/A    344792      C   ...ctrogram/.venv/bin/python     4375MiB |\n",
      "|    0   N/A  N/A   1598827      C   /usr/bin/python3                  675MiB |\n",
      "|    2   N/A  N/A   1515062      C   python                          10517MiB |\n",
      "|    3   N/A  N/A   1611734      C   python                           9119MiB |\n",
      "|    4   N/A  N/A    151424      C   python                           8749MiB |\n",
      "|    5   N/A  N/A    718402      C   ...rogram/env_aml/bin/python    10459MiB |\n",
      "|    6   N/A  N/A    151935      C   python                          10517MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6974c4-8202-4893-8ced-da97a33fd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv checkpoints/* wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4c870-d26f-41ce-9f5d-bd2ea94acab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7606d1f-cbce-4636-bf33-79b56c6d9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"model.h5\" is saved in wandb.run.dir & will be uploaded at the end of training\n",
    "model.save(os.path.join(wandb.run.dir, \"model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47683f13-f274-478d-a64c-0834d23866d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe3790-ffd9-40f3-8c76-10921e4a26e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a02dd3-d816-43d7-8ebc-70873551c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = WebNLGModel.load_from_checkpoint(\n",
    "   trainer.checkpoint_callback.best_model_path\n",
    ")\n",
    "\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24b571-499f-42ae-8eb1-ec1626a1ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c79ce-f296-4252-ba00-ccbe478fff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    text_encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=263,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    generated_ids = trained_model.model.generate(\n",
    "        input_ids=text_encoding[\"input_ids\"],\n",
    "        attention_mask=text_encoding[\"attention_mask\"],\n",
    "        max_length=147,\n",
    "        num_beams=2,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "        \n",
    "    preds = [\n",
    "        tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for gen_id in generated_ids\n",
    "    ]\n",
    "        \n",
    "    return \"\".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07372a6-2195-4e48-ae15-417e2a8db200",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = test_df.iloc[0]\n",
    "text = sample_row[\"input_text\"]\n",
    "model_summary = summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81122da3-6f94-48bc-b6c3-9cc5d2cb5b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777fc85-3eaf-4e52-a186-09a454d3570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row[\"target_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b798066-68ad-4b2d-ae76-e67c2636155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c02ae7-9456-4b74-9f7b-aaf118ee3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = test_df.iloc[2]\n",
    "text = sample_row[\"input_text\"]\n",
    "model_summary = summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a53d2-78d4-4563-a9b9-e97ad506f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9938134-0fe5-4699-9ccb-a1814ed802cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row[\"target_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5e04f-05e0-4e24-92c1-705da37e59fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5159afd-dafe-4631-b8e6-7a2d38c4af05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
